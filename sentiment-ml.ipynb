{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis With Machine Learning\n",
    "\n",
    "In this project, we'll try to utilize machine learning models with tfidf/countervectorizer words representation to do sentiment analysis. It's amazing that machine leaning algorithms  have better performance of 88.01% accuracy even beat deep learning models with glove word embeddings which is 81% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read\n",
    "Here we have a look at movies reviews and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "with open(\"data/reviews.txt\",'r') as f:\n",
    "    reviews = f.read()\n",
    "with open(\"data/labels.txt\",'r') as f:\n",
    "    labels = f.read()\n",
    "print (reviews[:1000])\n",
    "print (labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "Before we represent words as numerical data,we split and check reviews with zero length and remove it.\n",
    "1. The reviews are delimited with \\n , we'll split reviews by \\n\n",
    "2. Checking reviews for zero length and removing it.\n",
    "3. We explore reviews length and most common words to compare positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We split reviews and labels by \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = reviews.split('\\n')\n",
    "labels_list = labels.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We find one review with zero length and remove it as well as its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking null values:   0\n",
      "the zero-length review have index   [25000]\n",
      "Remove the zero-length review!\n"
     ]
    }
   ],
   "source": [
    "reviews_series = pd.Series([r for r in reviews_list])\n",
    "print (\"Checking null values:  \",sum(reviews_series.isna()))\n",
    "review_length = reviews_series.apply(lambda r:len(r))\n",
    "zero_index = [i for i,v in enumerate(review_length) if v==0]\n",
    "print (\"the zero-length review have index  \",zero_index)\n",
    "del reviews_list[25000]\n",
    "del labels_list[25000]\n",
    "print (\"Remove the zero-length review!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels:\n",
      " 0    1\n",
      "1    0\n",
      "2    1\n",
      "dtype: int64\n",
      "The total reviews is 25000.The ratio of positive reviews is 0.50 \n"
     ]
    }
   ],
   "source": [
    "reviews_series = reviews_series.drop(index=zero_index,axis=0,inplace=False)\n",
    "#print (reviews_series.shape)\n",
    "labels_series = pd.Series([l for l in labels_list]).map({'positive':1,'negative':0})\n",
    "print (\"Sample labels:\\n\",labels_series[:3])\n",
    "print (\"The total reviews is {}.The ratio of positive reviews is {:.2f} \".format(reviews_series.shape[0],labels_series.sum()/labels_series.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We explore and compare the reviews length for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bromwell high is a cartoon comedy . it ran at ...</td>\n",
       "      <td>832</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>667</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "      <td>2398</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "      <td>4476</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>brilliant over  acting by lesley ann warren . ...</td>\n",
       "      <td>857</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  review_length     label\n",
       "0  bromwell high is a cartoon comedy . it ran at ...            832  positive\n",
       "1  story of a man who has unnatural feelings for ...            667  negative\n",
       "2  homelessness  or houselessness as george carli...           2398  positive\n",
       "3  airport    starts as a brand new luxury    pla...           4476  negative\n",
       "4  brilliant over  acting by lesley ann warren . ...            857  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = pd.DataFrame(reviews_series,columns=[\"review\"])\n",
    "dt[\"review_length\"] = dt.review.apply(len)\n",
    "dt[\"label\"] = labels_list\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>1324.63800</td>\n",
       "      <td>969.383844</td>\n",
       "      <td>55.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>993.0</td>\n",
       "      <td>1595.0</td>\n",
       "      <td>9117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>1367.62336</td>\n",
       "      <td>1058.787192</td>\n",
       "      <td>74.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>13859.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count        mean          std   min    25%    50%     75%  \\\n",
       "label                                                                    \n",
       "negative  12500.0  1324.63800   969.383844  55.0  722.0  993.0  1595.0   \n",
       "positive  12500.0  1367.62336  1058.787192  74.0  707.0  998.0  1673.0   \n",
       "\n",
       "              max  \n",
       "label              \n",
       "negative   9117.0  \n",
       "positive  13859.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.groupby(\"label\").review_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see positive reviews are slightly longer than negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'review Length')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "dt[dt.label=='positive'].review_length.plot(bins=35, kind='hist', color='green', \n",
    "                                       label='positive', alpha=0.6)\n",
    "dt[dt.label=='negative'].review_length.plot(bins=35,kind='hist', color='red', \n",
    "                                       label='negative', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"review Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(review):\n",
    "    \"\"\"\n",
    "    Remove punctuations and stopwords in review.\n",
    "    Words stemming in review.\n",
    "    \n",
    "    Return:\n",
    "    list of cleaned words with lower case.\n",
    "    \"\"\"\n",
    "    nopunc = [char for char in review if char not in string.punctuation]#remove punctuations\n",
    "    nopunc = ''.join(nopunc)\n",
    "    review = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]#remove stopwords\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(w) for w in review]#word stemming\n",
    "    return ' '.join(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the most common words are both in positive and negative reviews, these are useless to predict labels, we'll remove these most common words in model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common 50 words in positive reviews:  [('br', 49235), ('film', 25309), ('movi', 22661), ('one', 14170), ('like', 10461), ('time', 8497), ('good', 7839), ('see', 7492), ('stori', 7481), ('charact', 7075), ('make', 6968), ('well', 6703), ('watch', 6554), ('great', 6489), ('get', 6476), ('love', 6167), ('show', 5590), ('also', 5550), ('realli', 5476), ('would', 5400), ('even', 5121), ('play', 5099), ('scene', 4994), ('first', 4756), ('much', 4685), ('end', 4660), ('peopl', 4547), ('way', 4532), ('best', 4323), ('think', 4295), ('go', 4249), ('life', 4207), ('look', 4178), ('year', 4055), ('work', 3968), ('made', 3823), ('mani', 3776), ('two', 3735), ('perform', 3653), ('know', 3612), ('thing', 3511), ('man', 3486), ('act', 3476), ('take', 3442), ('come', 3421), ('seen', 3415), ('still', 3363), ('littl', 3341), ('say', 3271), ('actor', 3251)]\n",
      "The most common 50 words in negtive reviews:   [('br', 52637), ('movi', 29046), ('film', 22894), ('one', 13572), ('like', 12340), ('make', 8242), ('even', 7791), ('time', 7697), ('get', 7664), ('good', 7524), ('bad', 7437), ('watch', 7391), ('charact', 7106), ('would', 7036), ('see', 6622), ('realli', 6262), ('look', 5875), ('stori', 5693), ('scene', 5602), ('act', 5323), ('much', 5078), ('go', 5057), ('end', 4992), ('peopl', 4845), ('thing', 4705), ('could', 4686), ('think', 4620), ('made', 4541), ('plot', 4364), ('well', 4336), ('first', 4306), ('way', 4297), ('show', 4286), ('say', 4187), ('seem', 4055), ('know', 3902), ('tri', 3847), ('want', 3693), ('play', 3633), ('actor', 3622), ('also', 3608), ('better', 3361), ('come', 3326), ('ever', 3271), ('seen', 3266), ('never', 3259), ('take', 3214), ('two', 3173), ('work', 3136), ('littl', 3096)]\n"
     ]
    }
   ],
   "source": [
    "dt[\"review_clean\"] = dt.review.apply(text_process)\n",
    "words_pos = dt[dt.label=='positive'].review_clean.apply(lambda x:[w for w in x.split()])\n",
    "words_pos_counter = Counter()\n",
    "for w in words_pos:\n",
    "    words_pos_counter.update(w)\n",
    "print (\"The most common 50 words in positive reviews: \",words_pos_counter.most_common(50))\n",
    "words_neg = dt[dt.label=='negative'].review_clean.apply(lambda x:[w for w in x.split()])\n",
    "words_neg_counter = Counter()\n",
    "for w in words_neg:\n",
    "    words_neg_counter.update(w)\n",
    "print (\"The most common 50 words in negtive reviews:  \",words_neg_counter.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Train\n",
    "\n",
    "We split the data into train and test set then use CountVectorizer and TfidfTransformer to vectorize words. We should fit and transform trainset and transform testset, so we have to do train_test_split at first. If we fit and transform on the whole dataset and then split them, the tfidf vectors will have both information on trainset and testset, that is data leakage. Be aware that if we fit on trainset, words only showing in testset will be disgarded when transform on testset. We could also use TfidfVectorizer to convert a collection of raw documents to a matrix of tfidf features. It combines the CountVectorizer and TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(dt.review_clean,dt.label,train_size = 0.6,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('CountVect',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=0.1,\n",
      "                                 max_features=None, min_df=2,\n",
      "                                 ngram_range=(1, 2), preprocessor=None,\n",
      "                                 stop_words='english', strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('TfidfTrans',\n",
      "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
      "                                  sublinear_tf=False, use_idf=True)),\n",
      "                ('MultinomialNB',\n",
      "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "pipeline1 = Pipeline([('CountVect',CountVectorizer(lowercase=True,stop_words='english',analyzer='word',ngram_range=(1,2))),\n",
    "                      ('TfidfTrans',TfidfTransformer()),\n",
    "                      ('MultinomialNB',MultinomialNB())])\n",
    "\n",
    "param = dict(CountVect__max_df=[0.1,0.2],CountVect__min_df=[2,5])\n",
    "\n",
    "gridsearch1 = GridSearchCV(pipeline1,param_grid=param,scoring='accuracy',cv=5)\n",
    "\n",
    "#print (gridsearch1.estimator.get_params().keys())\n",
    "gridsearch1.fit(X_train,y_train)\n",
    "print (gridsearch1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8742"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_predict = gridsearch1.predict(X_test)\n",
    "metrics.accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to remove tfidf transformer and get the slightly lower score than best estimator with tfidf, so we keep tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8729"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_onlycv = Pipeline([\n",
    "    ('CountVect',CountVectorizer(lowercase=True,stop_words='english',analyzer='word',ngram_range=(1,2),max_df=0.1,min_df=2)),\n",
    "    ('MultinomialNB',MultinomialNB())\n",
    "]\n",
    ")\n",
    "pipeline_onlycv.fit(X_train,y_train)\n",
    "y_predict = pipeline_onlycv.predict(X_test)\n",
    "metrics.accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test model with logisticregression ,and get a better performance on testset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8801"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lr = Pipeline([\n",
    "    ('CountVect',CountVectorizer(lowercase=True,stop_words='english',analyzer='word',ngram_range=(1,2),max_df=0.1,min_df=2)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('logisticreg',LogisticRegression(solver='liblinear'))\n",
    "]\n",
    ")\n",
    "pipeline_lr.fit(X_train,y_train)\n",
    "y_predict = pipeline_lr.predict(X_test)\n",
    "metrics.accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4276,  692],\n",
       "       [ 507, 4525]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = pd.Series(y_test).map({'positive':1,'negative':0})\n",
    "y_predict1 = pd.Series(y_predict).map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = y_test1.reset_index(drop=True) #reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['part of the enjoyment that i took from this film stemmed from the fact that i knew nothing more about it than that it starred john turturro and emily watson   reasons enough to watch   was a period piece and involved chess . everything that evolved before me was completely unexpected . i shan  t  therefore  give away much more . suffice to say that turturro is magnificent as an eccentric  obsessive and deeply vulnerable chess genius and em matches him step for step as the strong  minded woman who is drawn to him . it  s about love and obsession  rather than the venerated board game and after drawing me in gradually over the first half hour  became totally compelling . and i defy anyone to second  guess the ending .  '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_index = X_test.index\n",
    "np.array(dt.review.iloc[X_index])[(y_test1==1)&(y_test1==y_predict1)][2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predict error with true positive. \n",
      "\n",
      "['part of the enjoyment that i took from this film stemmed from the fact that i knew nothing more about it than that it starred john turturro and emily watson   reasons enough to watch   was a period piece and involved chess . everything that evolved before me was completely unexpected . i shan  t  therefore  give away much more . suffice to say that turturro is magnificent as an eccentric  obsessive and deeply vulnerable chess genius and em matches him step for step as the strong  minded woman who is drawn to him . it  s about love and obsession  rather than the venerated board game and after drawing me in gradually over the first half hour  became totally compelling . and i defy anyone to second  guess the ending .  ']\n",
      "\n",
      " Sample predict error with true negative. \n",
      "\n",
      "['the director tries to be quentin tarantino  the screenwriters try to be tennessee williams  deborah kara unger tries to be faye dunaway  the late james coburn tries to be orson welles  michael rooker tries to be gene hackman  mary tyler moore tries to be faye dunaway  older version   cameron diaz tries to get out of the frame as quickly as she can  successfully   don  t ask about joanna going . eric stoltz and james spader try to conceal their embarrassment with this crappy stuff . it delivers endless  meaningless dialog and very little action .  br    br   tulsa is a town with beautiful elevator lobbies  an art deco church by bruce goff and a lovely  sprawling mansion by frank lloyd wright . visit tulsa  don  t watch this movie . it doesn  t do the location justice .  ']\n"
     ]
    }
   ],
   "source": [
    "print (\"Sample predict error with true positive. \\n\")\n",
    "print (np.array(dt.review.iloc[X_index])[(y_test1==1)&(y_test1==y_predict1)][2:3])\n",
    "print (\"\\n Sample predict error with true negative. \\n\")\n",
    "print (np.array(dt.review.iloc[X_index])[(y_test1==0)&(y_test1==y_predict1)][2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predict error with false positive. \n",
      "\n",
      "['snow white  which just came out in locarno  where i had the chance to see it  of course refers to the world famous fairy tale . and it also refers to coke . in the end  real snow of the swiss alps plays its part as well .  br    br   thus all three aspects of the title are addressed in this film . there is a lot of dope on scene  and there is also a pale  dark haired girl  with a prince who has to go through all kind of trouble to come to her rescue .  br    br   but it  s not a fairy tale . it  s supposed to be a realistic drama located in zurich  switzerland  according to the tagline  .  br    br   technically the movie is close to perfect . unfortunately a weak plot  foreseeable dialogs  a mostly unreal scenery and the mixed acting don  t add up to create authenticity . thus as a spectator i remained untouched .  br    br   and then there were the clichs  which drove me crazy one by one snow white is a rich and spoiled upper class daughter  of course her parents are divorced and she never got enough love from them  because they were so busy all the time . her best girlfriend  on the other hand  has loving and caring parents . they  a steelworker and a housewife  live in a tiny flat  poor and happy  and ignorant of the desperate situation their daughter is in . the good guy   prince  is a musician    from the french speaking part of switzerland  which is considered to be the economically less successful but emotionally fitter fraction of the country  . he has problems with his parents . they are migrants from spain  who don  t seem to accept his wild way of living  until the father becomes seriously ill and confesses his great admiration for his son from a hospital bed .  br    br   and so it goes on naturally  the drug dealer is brutal  the bankers are heartless  the club owner is a playboy and the photographer  although a woman     has only her career in mind when she exposes snow white in artsy pornographic pictures at a show .  br    br   this review doesn  t need a spoiler in order to let you add these pieces to an obvious plot . as i like other films by samir  e . g .  forget baghdad   i was quite disappointed . let  s hope for the next one .  ']\n",
      "\n",
      " Sample predict error with false negative. \n",
      "\n",
      "['ok  i saw this in the theaters when it came out and i don  t know why . i haven  t seen it since  but i ended up on this page because i found myself thinking about this film  again i don  t know why . but the fact that i remember it speaks volumes .  br    br   comedy is hard  much harder than any drama . doing it right makes it seem easy  but doing it wrong . . . is there anything worse than a bad comedy  steve martin  pay attention  you are falling in this category again for some reason .  br    br   elvira mistress of the dark must have done it right for me to remember this movie fondly . done at a quick pace  with tongue in cheek and knowing it isn  t the philedelphia story  it entertains from start to finish . brain donors is another that fits right in this category  sans most of the gratuitous boob jokes  .  br    br   one point of contention  the ending . it seems the writers  director had no idea how to finish a comedy . the ending tries to be a love story  somewhat undermining the quirky fast  paced dialogue up to that point . then there is the  tassle  scene . whereas this has to be seen  male opinion   it is so over the top and out of place it  s like a shock . one more rewrite for the ending was needed .  br    br   this is not comedy genius like spinal tap or the producers or the holy grail . but if you don  t try to dissect it and just let the puns and sappy fun come to you  you  ll laugh  i guarantee it .  ']\n"
     ]
    }
   ],
   "source": [
    "print (\"Sample predict error with false positive. \\n\")\n",
    "print (np.array(dt.review.iloc[X_index])[y_test1<y_predict1][2:3])\n",
    "print (\"\\n Sample predict error with false negative. \\n\")\n",
    "print (np.array(dt.review.iloc[X_index])[y_test1>y_predict1][2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "Machine learning algorithms with tfidf/countervectorizer words representing also do a good job of 88.01% accuracy on sentiment analysis. Even better than lstm,cnn with glove word embeddings of about 81% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
